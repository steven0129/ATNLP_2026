{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd232b7",
   "metadata": {},
   "source": [
    "## Lab 1 â€“ Evaluating Language Models: hands-on with MMLU\n",
    "\n",
    "The objective of this lab session is for you to get familiar with autoregressive language models, in particular we will work with a historically relevant model: GPT-2. You can find the huggingface model card [here](https://huggingface.co/openai-community/gpt2). \n",
    "\n",
    "We are going to set up the basic code to _evaluate_ GPT-2 on a multiple choice question-answering task (MCQA). This will involve downloading the data (2.) and inspecting it so that we understand how to build a prompt (3.). In other Tutorials we will experiment a bit more on how different prompts interact with different models, so it is important to understand how to dynamically generate prompts, in this Lab we will introduce `langchain`'s `PromptTeplate`.\n",
    "\n",
    "Finally, there are different ways in which one could evaluate the model on MCQA. We will choose to implement one that compares the average log-probabilities over the tokens of the different possible answers (4.).\n",
    "\n",
    "The lab should be runnable on CPU (albeit slightly slow). You can try to use [google colab's](https://colab.research.google.com/) free tier GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb7baa",
   "metadata": {},
   "source": [
    "### 1. Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install datasets transformers langchain openai tiktoken==0.5.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e652d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "MODEL_NAME = \"gpt2\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c0928",
   "metadata": {},
   "source": [
    "### 2. Loading the dataset\n",
    "\n",
    "We are going to work with a dataset called Measuring Massive Multitask Language Understanding (MMLU), you can find the paper [here](https://arxiv.org/pdf/2009.03300).\n",
    "\n",
    "MMLU was designed to be a challenging evaluation for language models, to prevent the saturation of many datasets of the time, when the first large-scale models where appearing and making existing benchmarks obsolete. As such, this will be a difficult task for GPT-2!\n",
    "\n",
    "As we will see below, it is a dataset of multiple choice question answering (MCQA). As whenever one works with a new dataset, it is worth to spend some time looking at the data to get a feel for it!\n",
    "\n",
    "Go to huggingface and find the `cais/mmlu` dataset. Note down the structure of the dataset.\n",
    "\n",
    "We will work with the `high_school_biology` task for now. Note that this dataset does not have training split (they provide QA data from other datasets instead), so we will download just the `test` split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = load_dataset(\"cais/mmlu\", \"high_school_biology\", split=\"test\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a3c6b",
   "metadata": {},
   "source": [
    "### 3. From data to prompts\n",
    "\n",
    "We are going to use the `PromptTemplate` class of the `langchain` library to create prompts given a sample.\n",
    "\n",
    "The template you have to implement for now is: ```{question}\\n\\nA. {A}\\nB. {B}\\nC. {C}\\nD. {D}\\n\\nAnswer:```\n",
    "\n",
    "For the first sample in our high school biology split, your function should return\n",
    "\n",
    "```\n",
    "In a population of giraffes, an environmental change occurs that favors individuals that are tallest. As a result, more of the taller individuals are able to obtain nutrients and survive to pass along their genetic information. This is an example of\\n\\nA. directional selection.\\nB. stabilizing selection.\\nC. sexual selection.\\nD. disruptive selection.\\n\\nAnswer:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_MCQA_prompt(sample: dict) -> str:\n",
    "    \"\"\"Convert an MMLU sample into a multiple-choice prompt string using LangChain PromptTemplate.\n",
    "    The prompt contains the question followed by labeled choices: A., B., C., D., ...\n",
    "    \"\"\"\n",
    "    pass \n",
    "\n",
    "convert_to_MCQA_prompt(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328b821",
   "metadata": {},
   "source": [
    "### 4. Evaluating GPT-2\n",
    "\n",
    "Using this MCQA prompt, we want to evaluate GPT-2 by looking at the logits of each possible answer, when appended at the end of the prompt. We will use `AutoModelForCausalLLM` to load the model, and then, for each variation of MCQA_prompt + choice, average the log probabilities of the tokens in the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    # device_map=\"auto\" # uncomment this for GPU usage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b08c2",
   "metadata": {},
   "source": [
    "Given the MCQA prompt, there are different ways to extract an answer from an autoregressive language model. One would be to simply choose a sampling algorithm and decode one token, then compare if this token is A, B, C or D. You could try this, and think on what are the problems of this decoding approach. In this lab, we will use an alternative method, that will let us practice handling the outputs of an autoregressive language model like GPT-2.\n",
    "\n",
    "The idea is that we are going to 'force decode' the Prompt+answer pairs for each possible choice. Then, for each token on the choice, we can extract the logits, and convert those into log-probabilities using `torch.nn.functional.log_softmax`. At each position, this will be a token with the dimension of the vocabulary, and we need to find the log-probability of the appropiate token in the answer (through the token ID, which is the index of that token in the log-probabilities vector). Now we acerage, and that gives us a score for every answer.\n",
    "\n",
    "The recomendation is to start messing around with the outputs of the generation:\n",
    "```\n",
    "outputs = model(input_ids)\n",
    "logits = outputs.logits  # (1, seq_len, vocab_size)\n",
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "```\n",
    "and try to extract the probabilities of different tokens and understand the dimensions of these tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e18e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_gpt2_logits(prompt: str, choices: List[str], model, tokenizer, device=None) -> str:\n",
    "    \"\"\"Evaluate which choice (A/B/C/D) GPT-2 prefers using log-probabilities (sum of token log-probs) of the actual answer.\n",
    "    You should join the prompt which each possible choice, and average the logits over the tokens in the choice. Then return the logits for each option in a dictionary:\n",
    "    {'A': logp_choice_A, 'B': logp_B, 'C': logp_C, 'D': logp_D}\"\"\"\n",
    "    letters = ['A', 'B', 'C', 'D']\n",
    "    with torch.no_grad():\n",
    "        # Implement extracting the log-probabilities for each choice.\n",
    "        pass\n",
    "        #return {letters[i]: scores[i] for i in range(len(choices))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b844ec7",
   "metadata": {},
   "source": [
    "Now we can write a small loop to evaluate the model. \n",
    "\n",
    "- Can you think of any other ways of evaluating GPT-2 on a QA dataset?\n",
    "\n",
    "- How is the performance? Why is it that way?\n",
    "\n",
    "- Can you modify your code to be more efficient by batching the calculation?\n",
    "\n",
    "- Can you visualize the results using `matplotlib` and `seaborn`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7827dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for d in data:\n",
    "    prompt = convert_to_MCQA_prompt(d)\n",
    "    choices = list(d['choices']) if isinstance(d['choices'], (list, tuple)) else list(d['choices'].values())\n",
    "    pred_logits = evaluate_gpt2_logits(prompt, choices, model, tokenizer)\n",
    "    results.append({\n",
    "        'answer': ['A', 'B', 'C', 'D'][d['answer']],\n",
    "        # Key with the minimum log-probability (best choice)\n",
    "        'pred_logits': min(pred_logits, key=pred_logits.get)\n",
    "    })\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generic10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
