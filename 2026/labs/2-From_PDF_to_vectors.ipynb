{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7c34ad",
   "metadata": {},
   "source": [
    "## ATNLP Lab 2 – Towards retrieval-augmented generation: from a PDF to a vector database\n",
    "\n",
    "The objective of this tutorial is to build a basic system of semantic search or vector search over a pdf. We will build the utilities and data strcutures needed for, given a textual query, retrieve from a pdf document chunks of text that are relevant to it.\n",
    "\n",
    "* Extract text from a PDF.  \n",
    "* Split the text into moderately sized chunks (≈ 2–3 sentences each).  \n",
    "* Encode all chunks with a small SBERT model and persist the embeddings.  \n",
    "* Implement a tiny retrieval function that, given a user query, returns the most relevant chunk(s) of the original document.\n",
    "\n",
    "#### Vector databse vs Relational database\n",
    "\n",
    "A vector database is optimized for storing and querying high-dimensional data, like vectors used in AI for similarity searches. Unlike relational databases, which organize data into tables with rows and columns for structured data, vector databases focus on efficiently managing vector data, enabling fast nearest neighbor searches. While relational databases excel in handling structured data with clear relationships, vector databases are ideal for unstructured data where relationships are defined by vector proximity. \n",
    "\n",
    "In this tutorial, we will take a very simple approach, simply persisting the vectors in the disk. But at the end you can find some resources for open source vector databases that are used in production for many industry applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c05db1",
   "metadata": {},
   "source": [
    "### 1. Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers==2.2.2 pdfminer.six==20221105 nltk==3.8.1 scikit-learn==1.3.2 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "nltk.download('punkt')  # sentence tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9d150",
   "metadata": {},
   "source": [
    "### 2. Extracting text from a PDF\n",
    "\n",
    "We need to find a PDF to practice on. You can download a biology textbook from [this link](https://www.basicbiology.net/wp-content/uploads/edd/2018/05/Basic-Biology-an-introduction.pdf), save it on the `data/` folder. We will extract the raw text with a pdf utility & take a look at it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"data/Basic-Biology-an-introduction.pdf\"\n",
    "\n",
    "raw_text = extract_text(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw text length: {len(raw_text):,} characters\")\n",
    "print(raw_text[:100]) \n",
    "print(f\"\\nText in the middle: ...{raw_text[10000:10342]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b593ab",
   "metadata": {},
   "source": [
    "### 3. Post-process raw text into chunks\n",
    "\n",
    "The objective is to have small pieces of text that contain meaningul content for us to embed & do vector search over later on. Is the appropiate size a sentence? Maybe too little. How about a paragraph? How to detext paragraphs in the text we have?\n",
    "\n",
    "Here you should play around a bit with different ways of splitting the text of the pdf into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e69027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(\n",
    "    text: str,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split the raw text into chunks.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5c001",
   "metadata": {},
   "source": [
    "### 4. Embedding the chunks\n",
    "\n",
    "Now it is time to embed those chunks into vectors. We will use a small transformer model from the `sentence-transformers` lilbrary. I highly recommend you check their [website and documentation](https://sbert.net/), because they include many small blogs and explanations on how to train, evaluate & architect sentence encoders that I think are very well presented!\n",
    "\n",
    "Because we want the embedding process to be fast, implement batching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"all-MiniLM-L6-v2\"  # ~64 MB, 384-dimensional vectors\n",
    "model = SentenceTransformer(MODEL_NAME, device=\"cpu\") # if you have GPU available, play around with bigger models!\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encode a list of texts -> numpy matrix (n_chunks × dim).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "batch_size = 32  # adjust based on your RAM\n",
    "for batch_of_chunks in tqdm(...):  # etc\n",
    "    pass\n",
    "chunks=[]\n",
    "embeddings=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8999a76",
   "metadata": {},
   "source": [
    "### 5. Persist the data\n",
    "\n",
    "The end result should be a list of embeddings and a list of chunks, with matching indices. We will save the data compressed as .pkl. We build a helper function to decompress it and load it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4304d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"data/biology/\" \n",
    "with open(Path(save_folder) / \"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "with open(Path(save_folder) / \"embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d7a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_file(file_path: str):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af93ba",
   "metadata": {},
   "source": [
    "### 6. Build a simple retrieval function\n",
    "\n",
    "For an input sentence (or query), we need to embed it with the model, and perform a cosine similarity with the embeddings of the textbook. Then, we return the top3 chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea992a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = unpickle_file(\"data/biology/embeddings.pkl\")\n",
    "chunks = unpickle_file(\"data/biology/chunks.pkl\")\n",
    "\n",
    "def cosine_similarity(vector_1, vector_2):\n",
    "    pass\n",
    "\n",
    "def search(\n",
    "        query:str,\n",
    "        embeddings: np.ndarray,\n",
    "        chunks: List[str],\n",
    "        model: SentenceTransformer,\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Given a query, return the top 5 most similar chunks with similarity scores.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be666ab5",
   "metadata": {},
   "source": [
    "### 7. Final thoughts\n",
    "\n",
    "Although good to grasp the basics, this is a very basic implementation of an extremely powerful tool: vector search & vector databases. If you have a bit of time & want to push this a bit further, I would recommend taking a look at [Qdrant](https://qdrant.tech/), a startup that implements open-source vector databases to do what we just did, but at scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generic10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
